---
title: "Practical Machine Learning Course Project"
author: "Jared Brooks"
date: "2/24/2018"
output:
  md_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Human Activity Recognition

We are using exercise tech data to try to classify different types of activities. In this dataset, six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. We are tasked with cleaning and preprocessing this data in order to fit a machine learning model to predict which of the 5 different ways participants are performing the activity.

### Reading in the Data

```{r open}
training = read.csv("~/Downloads/pml-training.csv")
testing = read.csv("~/Downloads/pml-testing.csv")
str(training, list.len=20)
```

### Data Cleaning

Here we output the structure of the first 20 columns of the training set. We use this command, along with summary and some histograms to see that many of the columns consist of values that are either mostly NA or \'' (empty strings). To clean the training set we remove these columns, along with removing the corresponding columns from the test set.

We also see that the first column is simply an index, so we don't need that. Furthermore, the 6th column, 'new_window', seems to not be useful for this classification problem.

```{r clean, results='hide'}
f1 <-  function(name) if((sum(is.na(training[name]))<19000)) {print(name)}
notNAnames <- unlist(lapply(names(training), f1))
training <- training[,notNAnames]
notNAnames.test <- notNAnames[-93]
testing <- testing[,notNAnames.test]

f2 <-  function(name) if((sum(training[name]=='')<19000)) {print(name)}
notEmptynames <- unlist(lapply(names(training), f2))
training <- training[,notEmptynames]
notEmptynames.test <- notEmptynames[-60]
testing <- testing[,notEmptynames.test]

training <- training[,-c(1,6)]
testing <- testing[,-c(1,6)]
```

```{r dim}
dim(training)
```

### Preprocessing (PCA)

Here we see that the cleaning process has reduced the number of columns from 160 to only 58, which is still large. We can further reduce the number of features to use by performing a Principle Components Analysis. After some exploratory testing, we found that using the principle components that account for 80% of the variance is sufficient for this problem.

```{r pca}
suppressMessages(library(caret))
train_num <- data.frame(sapply(training, function(x) as.numeric(x)))
train_num$classe <- as.factor(train_num$classe)
preObj <- preProcess(train_num[,-58], method = "pca", thresh=0.8)
pca_training_preds <- predict(preObj, train_num[,-58])
dim(training)
dim(pca_training_preds)
```

Here we see that only 13 principle components out of 57 (excluding the variable we are trying to predict) are necessary to account for 80% of the variance.

### Model Selection

We tried a number of different classification algorithms, including Naive Bayes and Support Vector Machines, but found that a Random Forest does the best job (Also, using train(...,method='rf') took significantly longer than randomForest(...), so that's the method we use). Below we show 3 crossfolds from random forest models using the top 11 principle components. And since we aren't using train(), we can't use it's built-in cross-validations routines.

### Cross Validation

```{r cv}
suppressMessages(library(caret))
suppressMessages(library(randomForest))

set.seed(39592)
fold1 <- createDataPartition(training$classe, p=0.8, list = F)
trainfold1 <- train_num[fold1,]
testfold1 <- train_num[-fold1,]
preObj1 <- preProcess(trainfold1[,-58], method = "pca", thresh=0.8)
pca_training_preds1 <- predict(preObj1, trainfold1[,-58])
pca_training_preds1$y <- trainfold1$classe
pca_testing_preds1 <- predict(preObj1, testfold1[,-58])
mod_rf_fold1 <- randomForest(y~.,data = pca_training_preds1)
pred_fold1 <- predict(mod_rf_fold1, pca_testing_preds1)
print(confusionMatrix(pred_fold1, testfold1$classe)$overall[1])

set.seed(40639)
fold2 <- createDataPartition(training$classe, p=0.8, list = F)
trainfold2 <- train_num[fold2,]
testfold2 <- train_num[-fold2,]
preObj2 <- preProcess(trainfold2[,-58], method = "pca", thresh=0.8)
pca_training_preds2 <- predict(preObj2, trainfold2[,-58])
pca_training_preds2$y <- trainfold2$classe
pca_testing_preds2 <- predict(preObj2, testfold2[,-58])
mod_rf_fold2 <- randomForest(y~.,data = pca_training_preds2)
pred_fold2 <- predict(mod_rf_fold2, pca_testing_preds2)
print(confusionMatrix(pred_fold2, testfold2$classe)$overall[1])

set.seed(94823)
fold3 <- createDataPartition(training$classe, p=0.8, list = F)
trainfold3 <- train_num[fold3,]
testfold3 <- train_num[-fold3,]
preObj3 <- preProcess(trainfold3[,-58], method = "pca", thresh=0.8)
pca_training_preds3 <- predict(preObj3, trainfold3[,-58])
pca_training_preds3$y <- trainfold3$classe
pca_testing_preds3 <- predict(preObj3, testfold3[,-58])
mod_rf_fold3 <- randomForest(y~.,data = pca_training_preds3)
pred_fold3 <- predict(mod_rf_fold3, pca_testing_preds3)
print(confusionMatrix(pred_fold3, testfold3$classe)$overall[1])
```

### Dependence on PCA Threshold

Here we plot the change in the number of PCs used and the error (1-accuracy) in a test set as we change the variation threshold for PCs. As the threshold increases, the necessary number of PC increases, but the error decreases.

```{r plot, echo=FALSE}
library(ggplot2)
pca_training_preds$y <- training$classe
threshs <- seq(0.5, 0.95, by=0.025)
pc_num <- c()
acc <- c()
for (thresh in threshs){
  set.seed(thresh)
  
  preO <- preProcess(train_num[,-58], method = "pca", thresh=thresh)
  plot_preds <- predict(preO, train_num[,-58])
  plot_preds$y <- training$classe
  
  fold <- createDataPartition(pca_training_preds$y, p=0.8, list = F)
  trainfold <- plot_preds[fold,]
  testfold <- plot_preds[-fold,]
  
  mod_rf_fold <- randomForest(y~.,data = trainfold)
  pred_plot <- predict(mod_rf_fold, testfold)
  
  acc <- append(acc, confusionMatrix(pred_plot, testfold$y)$overall[[1]])
  pc_num <- append(pc_num, dim(plot_preds)[2])
}

acc <- unlist(acc)
err <- 1-acc

df <- data.frame(threshs=threshs,pc_num=pc_num,err=err)

p <- ggplot(aes(x=threshs),data=df) + geom_line(aes(y = pc_num, colour = "Number of PCs"))
p <- p + geom_line(aes(y=err*200, colour = "Error on Training"))
p <- p + scale_y_continuous(sec.axis = sec_axis(~./200, name = "Error on Training"))
p <- p + scale_colour_manual(values = c("blue", "red"))
p <- p + labs(y = "Number of PCs", x = "Thresholds", colour = "Legend")
p <- p + theme(legend.position = c(0.8, 0.9))
p
```

### Test Set Predictions

```{r test}
test_num <- data.frame(sapply(testing, function(x) as.numeric(x)))
pca_testing_preds <- predict(preObj, test_num)

pca_training_preds$y <- training$classe
mod_rf_full <- randomForest(y~.,data = pca_training_preds)
pred <- predict(mod_rf_full, pca_testing_preds)
pred
```

Since the model does well consistently on the cross-validation folds, and because we used PCA, I think the model will do quite well on unseen data, probably >90% (out of sample error <10%).
